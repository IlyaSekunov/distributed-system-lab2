# Отчет по реализации задачи N тел на CUDA

## 1. Описание алгоритма

### 1.1. Физическая модель
Программа решает задачу N тел, используя закон всемирного тяготения Ньютона. Для каждой материальной точки вычисляются силы притяжения со стороны всех остальных точек согласно формуле:

**F**ₖq = G·(mₖ·m_q/|rₖ - r_q|³)·(rₖ - r_q)

где:
- G = 6.67430×10⁻¹¹ м³·кг⁻¹·с⁻² — гравитационная постоянная
- mₖ, m_q — массы тел
- rₖ, r_q — радиус-векторы тел

### 1.2. Численный метод
Для интегрирования уравнений движения используется метод Эйлера (явный):
1. Вычисление ускорений всех тел на основе текущих позиций
2. Обновление скоростей: v_new = v_old + a·Δt
3. Обновление позиций: r_new = r_old + v_new·Δt

Шаг интегрирования Δt = 3600 секунд (1 час).

### 1.3. Регуляризация
Для предотвращения сингулярности при сближении тел используется параметр смягчения (softening):
```
dist_sq = dx*dx + dy*dy + dz*dz + SOFTENING
```
где SOFTENING = 1×10⁻¹⁰.

## 2. Реализация на CUDA

### 2.1. Структура данных
```c
typedef struct {
    float mass;     // масса
    float x, y, z;  // координаты
    float vx, vy, vz; // скорости
    float ax, ay, az; // ускорения
} Body;
```

### 2.2. Основные ядра (kernels)

#### 2.2.1. `compute_accelerations_kernel`
- **Назначение**: вычисление ускорений для всех тел
- **Параллелизм**: каждое тело обрабатывается отдельным потоком
- **Особенности**: O(N²) сложность, каждый поток читает данные всех тел из глобальной памяти

#### 2.2.2. `update_positions_kernel`
- **Назначение**: обновление скоростей и позиций по методу Эйлера
- **Параллелизм**: каждое тело обрабатывается отдельным потоком

## 3. Методика измерений

### 3.1. Аппаратная платформа
- **Платформа**: Google Colab с GPU средрй выполнения
- **GPU**: NVIDIA Tesla T4
- **CUDA ядер**: 2560
- **CUDA версия**: 11.6
- **Компилятор**: nvcc с флагами оптимизации -O2

### 3.2. Параметры тестирования
- Количество тел (n): 30, 50, 100, 1000, 10000
- Размер блока (p): 32, 64, 128, 256
- Время симуляции: 1 день (86400 секунд)
- Количество шагов: 24 шага по 1 часу

### 3.3. Методика замера времени
1. Время измеряется с помощью CUDA Events
2. Измеряется полное время выполнения симуляции (все 24 шага)
3. **Для каждого сочетания (n, p) выполняется 5 запусков**
4. **В отчете представлено минимальное время из 5 запусков**

Отлично! С реальными данными хостовой программы можно сделать качественное сравнение. Вот обновленный раздел отчета с вашими данными:

## 4. Результаты производительности и сравнение

### 4.1. Таблица результатов GPU (время в миллисекундах)

| n\p  | 32    | 64    | 128   | 256   | **Лучший GPU** |
|------|-------|-------|-------|-------|----------------|
| 30   | 2.87  | 2.71  | 2.73  | 2.74  | **2.71** (p=64) |
| 50   | 3.608 | 3.33  | 3.37  | 3.36  | **3.33** (p=64) |
| 100  | 6.152 | 7.66  | 5.77  | 5.69  | **5.69** (p=256) |
| 1000 | 52.2  | 51.66 | 47.83 | 48.16 | **47.83** (p=128) |
| 10000| 476.0 | 456.2 | 463.6 | 465.1 | **456.2** (p=64) |

### 4.2. Результаты хостовой программы (CPU)

| n   | Время (мс) | Примечание          |
|-----|------------|---------------------|
| 30  | 1.998      | Последовательная    |
| 50  | 3.000      | Последовательная    |
| 100 | 6.000      | Последовательная    |
| 1000| 251.000    | Последовательная    |
| 10000| 99595.000  | Последовательная    |

### 4.3. Сравнение производительности GPU vs CPU

#### 4.3.1. Таблица сравнения (лучший GPU vs CPU):

| n   | CPU (мс) | GPU (мс) | Ускорение | Оптимальный p |
|-----|----------|----------|-----------|---------------|
| 30  | 1.998    | 2.71     | **0.74x** (CPU быстрее) | 64  |
| 50  | 3.000    | 3.33     | **0.90x** (CPU быстрее) | 64  |
| 100 | 6.000    | 5.69     | **1.05x** (GPU быстрее) | 256 |
| 1000| 251.000  | 47.83    | **5.25x** (GPU быстрее) | 128 |
| 10000| 99595.000| 456.2    | **218.3x** (GPU намного быстрее) | 64 |

### 4.4. Подробный анализ результатов

#### 4.4.1. Переломный момент производительности
- **n < 100**: CPU показывает сопоставимую или лучшую производительность
- **n = 100**: примерное равенство производительности (порог перехода)
- **n > 100**: GPU демонстрирует растущее преимущество

#### 4.4.2. Причины преимущества CPU при малых n:
1. **Накладные расходы CUDA**: запуск ядер, передача данных CPU↔GPU
2. **Недозагрузка GPU**: мало работы для 2560 CUDA ядер Tesla T4
3. **Локальность данных**: CPU кэши эффективнее при маленьких наборах данных

#### 4.4.3. Причины преимущества GPU при больших n:
1. **Массовый параллелизм**: O(N²) задача идеально масштабируется на GPU
2. **Высокая пропускная способность памяти**: 320 GB/s у Tesla T4
3. **Полная утилизация**: при n=10000 загружены все SM и CUDA ядра

#### 4.4.4. Экстремальный случай n=10000:
- **Абсолютное ускорение**: 218.3 раза
- **Время CPU**: ~99.6 секунд
- **Время GPU**: ~0.456 секунд
- **Экономия времени**: 99.1 секунды (более 99% времени)

## 5. Заключение по производительности

### 5.1. Ключевые выводы:
1. **Порог эффективности**: GPU становится эффективнее CPU при n ≥ 100
2. **Экстремальное ускорение**: для n=10000 достигается ускорение в 218 раз
3. **Оптимальные параметры**: p=64-128 для большинства случаев
4. **Масштабируемость**: GPU демонстрирует существенно лучшую масштабируемость при увеличении n

### 5.2. Практическая значимость:
- **Научные расчеты**: для моделирования планетарных систем, галактик (n > 1000) GPU незаменим
- **Образовательные цели**: для демонстрационных задач (n < 100) CPU достаточен
- **Производственные системы**: гибридный подход в зависимости от размера задачи

### 5.3. Дальнейшие улучшения:
1. **Динамический выбор**: автоматическое определение оптимальной платформы (CPU/GPU)
2. **Адаптивный размер блока**: автоматическая настройка p в зависимости от n
3. **Смешанная точность**: использование Tensor Cores Tesla T4 для further ускорения

**Итог**: Реализация успешно демонстрирует преимущества GPU вычислений для задачи N тел, особенно для больших систем, где достигается ускорение более чем в 200 раз по сравнению с последовательной CPU реализацией.